{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version history\n",
    "- V12 - (ENet B7 + DenseNet, best_alpha=0.5) + AugMix + part of Oxford Dataset **(0.96564)**\n",
    "- V15 - (ENet B7 + DenseNet, best_alpha=0.44) + AugMix + part of Oxford Dataset (0.96194)\n",
    "- V16 - (ENet B7 + ResNet152V2, best_alpha=0.5) + AugMix + part of Oxford Dataset (0.96013)\n",
    "- V17 - (V12), EPOCHS 20-> 25 (0.96404)\n",
    "- V19 - (V12), ENet B7 weights -> noisy-student, no normalization, with validation (0.95663)\n",
    "- V20 - (V19) with normalization and validation (0.95331)\n",
    "- V21 - (V19) no validation **(0.96445)**\n",
    "- V22 - (V21), ENet B7 weights -> imagenet, remove custom params, 30 epochs (0.96375)\n",
    "- V23 - (V22), Enet for 25 epochs, DenseNet for 20 epochs, kick up augmix level and add normalization (0.96170)\n",
    "\n",
    "\n",
    "IDEAS: grid mask + cutmix ? \n",
    "加上标签平滑？\n",
    "tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logit, label_smoothing=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "def measure_time(exec_history, event):\n",
    "    exec_history.append([event, time.time()-start_time])\n",
    "    return exec_history\n",
    "\n",
    "exec_history = measure_time([], \"Start notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import math, re, os\n",
    "import tensorflow as tf, tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import efficientnet.tfkeras as efn\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path('flower-classification-with-tpus') # you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"\n",
    "#OXFORD_PATH = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')\n",
    "#OXFORD_PATH = KaggleDatasets().get_gcs_path('oxford-102-for-tpu-competition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE_SIZE = [512, 512] # at this size, a GPU will run out of memory. Use the TPU\n",
    "IMAGE_SIZE = [331, 331]\n",
    "EPOCHS = 25\n",
    "\n",
    "BATCH_SIZE = 24 * strategy.num_replicas_in_sync\n",
    "\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n",
    "    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n",
    "    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n",
    "    #512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n",
    "}\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n",
    "TRAINING_FILENAMES_model2 = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n",
    "\n",
    "#tf.io.gfile.glob(OXFORD_PATH + '/*.tfrec')\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') # predictions on this dataset should be submitted for the competition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nGCS_PATH_SELECT = { # available image sizes\\n    \\n    192: GCS_DS_PATH +'/openimage/tfrecords-jpeg-192x192/',\\n    224: GCS_DS_PATH + '/openimage/tfrecords-jpeg-224x224/',\\n    331: GCS_DS_PATH + '/openimage/tfrecords-jpeg-331x331/',\\n    #512: GCS_DS_PATH + '/inaturalist/tfrecords-jpeg-512x512/',\\n    \\n}\\n\\nGCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\\n\\nTRAINING_FILENAMES_model2 = TRAINING_FILENAMES_model2 + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCS_DS_PATH =KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')\n",
    "\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    \n",
    "    192: GCS_DS_PATH +'/oxford_102/tfrecords-jpeg-192x192/',\n",
    "    224: GCS_DS_PATH + '/oxford_102/tfrecords-jpeg-224x224/',\n",
    "    331: GCS_DS_PATH + '/oxford_102/tfrecords-jpeg-331x331/',\n",
    "      \n",
    "}\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES_model2 = TRAINING_FILENAMES_model2 + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\n",
    "\n",
    "\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "\n",
    "    192: GCS_DS_PATH +'/tf_flowers/tfrecords-jpeg-192x192/',\n",
    "    224: GCS_DS_PATH + '/tf_flowers/tfrecords-jpeg-224x224/',\n",
    "    331: GCS_DS_PATH + '/tf_flowers/tfrecords-jpeg-331x331/',\n",
    "    #512: GCS_DS_PATH + '/tf_flowers/tfrecords-jpeg-512x512/',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES_model2 = TRAINING_FILENAMES_model2 + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\n",
    "\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    \n",
    "    192: GCS_DS_PATH +'/inaturalist/tfrecords-jpeg-192x192/',\n",
    "    224: GCS_DS_PATH + '/inaturalist/tfrecords-jpeg-224x224/',\n",
    "    331: GCS_DS_PATH + '/inaturalist/tfrecords-jpeg-331x331/'\n",
    "}\n",
    "\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES_model2 = TRAINING_FILENAMES_model2 + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\n",
    "\n",
    "\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    \n",
    "    192: GCS_DS_PATH +'/inaturalist_1/tfrecords-jpeg-192x192/',\n",
    "    224: GCS_DS_PATH + '/inaturalist_1/tfrecords-jpeg-224x224/',\n",
    "    331: GCS_DS_PATH + '/inaturalist_1/tfrecords-jpeg-331x331/',\n",
    "    #512: GCS_DS_PATH + '/inaturalist/tfrecords-jpeg-512x512/',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES_model2 = TRAINING_FILENAMES_model2 + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\n",
    "'''\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    \n",
    "    192: GCS_DS_PATH +'/openimage/tfrecords-jpeg-192x192/',\n",
    "    224: GCS_DS_PATH + '/openimage/tfrecords-jpeg-224x224/',\n",
    "    331: GCS_DS_PATH + '/openimage/tfrecords-jpeg-331x331/',\n",
    "    #512: GCS_DS_PATH + '/inaturalist/tfrecords-jpeg-512x512/',\n",
    "    \n",
    "}\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES_model2 = TRAINING_FILENAMES_model2 + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_DS_PATH =KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')\n",
    "\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    \n",
    "    192: GCS_DS_PATH +'/oxford_102/tfrecords-jpeg-192x192/',\n",
    "    224: GCS_DS_PATH + '/oxford_102/tfrecords-jpeg-224x224/',\n",
    "    331: GCS_DS_PATH + '/oxford_102/tfrecords-jpeg-331x331/',\n",
    "    #512: GCS_DS_PATH + '/inaturalist/tfrecords-jpeg-512x512/',   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES = TRAINING_FILENAMES + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\n",
    "\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    \n",
    "    192: GCS_DS_PATH +'/inaturalist/tfrecords-jpeg-192x192/',\n",
    "    224: GCS_DS_PATH + '/inaturalist/tfrecords-jpeg-224x224/',\n",
    "    331: GCS_DS_PATH + '/inaturalist/tfrecords-jpeg-331x331/',\n",
    "    #512: GCS_DS_PATH + '/inaturalist/tfrecords-jpeg-512x512/',\n",
    "    \n",
    "}\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES = TRAINING_FILENAMES + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    \n",
    "    192: GCS_DS_PATH +'/inaturalist_1/tfrecords-jpeg-192x192/',\n",
    "    224: GCS_DS_PATH + '/inaturalist_1/tfrecords-jpeg-224x224/',\n",
    "    331: GCS_DS_PATH + '/inaturalist_1/tfrecords-jpeg-331x331/',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES = TRAINING_FILENAMES + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    \n",
    "    331: GCS_DS_PATH + '/imagenet/tfrecords-jpeg-331x331/'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES = TRAINING_FILENAMES + tf.io.gfile.glob(GCS_PATH + '*.tfrec')\n",
    "# VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n",
    "# TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec')\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    \n",
    "    192: GCS_DS_PATH +'/tf_flowers/tfrecords-jpeg-192x192/',\n",
    "    224: GCS_DS_PATH + '/tf_flowers/tfrecords-jpeg-224x224/',\n",
    "    331: GCS_DS_PATH + '/tf_flowers/tfrecords-jpeg-331x331/',\n",
    "    512: GCS_DS_PATH + '/tf_flowers/tfrecords-jpeg-512x512/',\n",
    "}\n",
    "\n",
    "\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES = TRAINING_FILENAMES + tf.io.gfile.glob(GCS_PATH + '*.tfrec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 是否加入验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_VALIDATION = True#True#True #False True\n",
    "if SKIP_VALIDATION:\n",
    "    TRAINING_FILENAMES = TRAINING_FILENAMES + VALIDATION_FILENAMES\n",
    "    TRAINING_FILENAMES_model2 = TRAINING_FILENAMES_model2 + VALIDATION_FILENAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n",
    "           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n",
    "           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n",
    "           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n",
    "           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n",
    "           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n",
    "           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n",
    "           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n",
    "           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n",
    "           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n",
    "           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Import libraries, initilialize TPU and config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
    "        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    idnum = example['id']\n",
    "    return image, idnum # returns a dataset of image(s)\n",
    "\n",
    "def load_dataset(filenames, labeled = True, ordered = False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
    "    \n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "        \n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def data_augment(image, label):  # 训练样本在预读过程中进行数据增强，这部分操作在CPU上\n",
    "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
    "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
    "    # of the TPU while the TPU itself is computing gradients.\n",
    "    image = tf.image.random_flip_left_right(image,seed=42)\n",
    "    #image = dataset.map(decoded_example)\n",
    "    #image = augmentation(image)\n",
    "    \n",
    "    #image = tf.image.random_saturation(image, 0.4, 2.4)\n",
    "    return image, label\n",
    "\n",
    "def data_augment_1(image, label):  # 训练样本在预读过程中进行数据增强，这部分操作在CPU上\n",
    "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
    "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
    "    # of the TPU while the TPU itself is computing gradients.\n",
    "    image = tf.image.random_flip_left_right(image,seed=42)\n",
    "    #image = dataset.map(decoded_example)\n",
    "    #image = augmentation(image)\n",
    "    \n",
    "    #image = tf.image.random_saturation(image, 0.4, 2.4)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "def get_training_dataset():\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
    "    dataset = dataset.map(data_augment_1, num_parallel_calls=AUTO)\n",
    "    #\n",
    "    #dataset = dataset.map(decoded_example)\n",
    "    #dataset = dataset.map(augmentation)\n",
    "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_training_dataset_model2():\n",
    "    dataset = load_dataset(TRAINING_FILENAMES_model2, labeled=True)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
    "    #\n",
    "    #dataset = dataset.map(decoded_example)\n",
    "    #dataset = dataset.map(augmentation)\n",
    "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(ordered=False):\n",
    "    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE) # slighly faster with fixed tensor sizes\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(ordered=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and matplotlib defaults\n",
    "np.set_printoptions(threshold=15, linewidth=80)\n",
    "\n",
    "def batch_to_numpy_images_and_labels(data):\n",
    "    images, labels = data \n",
    "    numpy_images = images.numpy()\n",
    "    numpy_labels = labels.numpy()\n",
    "    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n",
    "        numpy_labels = [None for _ in enumerate(numpy_images)]\n",
    "    # If no labels, only image IDs, return None for labels (this is the case for test data)\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "def title_from_label_and_target(label, correct_label):\n",
    "    if correct_label is None:\n",
    "        return CLASSES[label], True\n",
    "    correct = (label == correct_label)\n",
    "    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n",
    "                                CLASSES[correct_label] if not correct else ''), correct\n",
    "\n",
    "def display_one_flower(image, title, subplot, red=False, titlesize=16):\n",
    "    plt.subplot(*subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "    \n",
    "def display_batch_of_images(databatch, predictions=None, figsize  = 13.0):\n",
    "    \"\"\"This will work with:\n",
    "    display_batch_of_images(images)\n",
    "    display_batch_of_images(images, predictions)\n",
    "    display_batch_of_images((images, labels))\n",
    "    display_batch_of_images((images, labels), predictions)\n",
    "    \"\"\"\n",
    "    # data\n",
    "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
    "    if labels is None:\n",
    "        labels = [None for _ in enumerate(images)]\n",
    "        \n",
    "    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images)//rows\n",
    "        \n",
    "    # size and spacing\n",
    "    FIGSIZE =  figsize\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols,1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "    \n",
    "    # display\n",
    "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
    "        title = '' if label is None else CLASSES[label]\n",
    "        correct = True\n",
    "        if predictions is not None:\n",
    "            title, correct = title_from_label_and_target(predictions[i], label)\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n",
    "        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n",
    "    \n",
    "    #layout\n",
    "    plt.tight_layout()\n",
    "    if label is None and predictions is None:\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    else:\n",
    "        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()\n",
    "\n",
    "def display_confusion_matrix(cmat, score, precision, recall):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(cmat, cmap='Reds')\n",
    "    ax.set_xticks(range(len(CLASSES)))\n",
    "    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    ax.set_yticks(range(len(CLASSES)))\n",
    "    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    titlestring = \"\"\n",
    "    if score is not None:\n",
    "        titlestring += 'f1 = {:.3f} '.format(score)\n",
    "    if precision is not None:\n",
    "        titlestring += '\\nprecision = {:.3f} '.format(precision)\n",
    "    if recall is not None:\n",
    "        titlestring += '\\nrecall = {:.3f} '.format(recall)\n",
    "    if len(titlestring) > 0:\n",
    "        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n",
    "    plt.show()\n",
    "    \n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一种数据增强方式\n",
    "AugParams = {\n",
    "    'd1' : 90,\n",
    "    'd2': 160,\n",
    "    'rotate' : 280,\n",
    "    'ratio' : 0.45\n",
    "    }\n",
    "\n",
    "\n",
    "def transform(image, inv_mat, image_shape):\n",
    "\n",
    "      h, w, c = image_shape\n",
    "      cx, cy = w//2, h//2\n",
    "\n",
    "      new_xs = tf.repeat( tf.range(-cx, cx, 1), h)\n",
    "      new_ys = tf.tile( tf.range(-cy, cy, 1), [w])\n",
    "      new_zs = tf.ones([h*w], dtype=tf.int32)\n",
    "\n",
    "      old_coords = tf.matmul(inv_mat, tf.cast(tf.stack([new_xs, new_ys, new_zs]), tf.float32))\n",
    "      old_coords_x, old_coords_y = tf.round(old_coords[0, :] + w//2), tf.round(old_coords[1, :] + h//2)\n",
    "\n",
    "      clip_mask_x = tf.logical_or(old_coords_x<0, old_coords_x>w-1)\n",
    "      clip_mask_y = tf.logical_or(old_coords_y<0, old_coords_y>h-1)\n",
    "      clip_mask = tf.logical_or(clip_mask_x, clip_mask_y)\n",
    "\n",
    "      old_coords_x = tf.boolean_mask(old_coords_x, tf.logical_not(clip_mask))\n",
    "      old_coords_y = tf.boolean_mask(old_coords_y, tf.logical_not(clip_mask))\n",
    "      new_coords_x = tf.boolean_mask(new_xs+cx, tf.logical_not(clip_mask))\n",
    "      new_coords_y = tf.boolean_mask(new_ys+cy, tf.logical_not(clip_mask))\n",
    "\n",
    "      old_coords = tf.cast(tf.stack([old_coords_y, old_coords_x]), tf.int32)\n",
    "      new_coords = tf.cast(tf.stack([new_coords_y, new_coords_x]), tf.int64)\n",
    "      rotated_image_values = tf.gather_nd(image, tf.transpose(old_coords))\n",
    "      rotated_image_channel = list()\n",
    "      for i in range(c):\n",
    "          vals = rotated_image_values[:,i]\n",
    "          sparse_channel = tf.SparseTensor(tf.transpose(new_coords), vals, [h, w])\n",
    "          rotated_image_channel.append(tf.sparse.to_dense(sparse_channel, default_value=0, validate_indices=False))\n",
    "\n",
    "      return tf.transpose(tf.stack(rotated_image_channel), [1,2,0])\n",
    "\n",
    "def random_rotate(image, angle, image_shape):\n",
    "\n",
    "    def get_rotation_mat_inv(angle):\n",
    "          #transform to radian\n",
    "          angle = math.pi * angle / 180\n",
    "\n",
    "          cos_val = tf.math.cos(angle)\n",
    "          sin_val = tf.math.sin(angle)\n",
    "          one = tf.constant([1], tf.float32)\n",
    "          zero = tf.constant([0], tf.float32)\n",
    "\n",
    "          rot_mat_inv = tf.concat([cos_val, sin_val, zero,\n",
    "                                     -sin_val, cos_val, zero,\n",
    "                                     zero, zero, one], axis=0)\n",
    "          rot_mat_inv = tf.reshape(rot_mat_inv, [3,3])\n",
    "\n",
    "          return rot_mat_inv\n",
    "    angle = float(angle) * tf.random.normal([1],dtype='float32')\n",
    "    rot_mat_inv = get_rotation_mat_inv(angle)\n",
    "    return transform(image, rot_mat_inv, image_shape)\n",
    "\n",
    "\n",
    "def GridMask(image_height, image_width, d1, d2, rotate_angle=1, ratio=0.5):\n",
    "\n",
    "    h, w = image_height, image_width\n",
    "    hh = int(np.ceil(np.sqrt(h*h+w*w)))\n",
    "    hh = hh+1 if hh%2==1 else hh\n",
    "    d = tf.random.uniform(shape=[], minval=d1, maxval=d2, dtype=tf.int32)\n",
    "    l = tf.cast(tf.cast(d,tf.float32)*ratio+0.5, tf.int32)\n",
    "\n",
    "    st_h = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n",
    "    st_w = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n",
    "\n",
    "    y_ranges = tf.range(-1 * d + st_h, -1 * d + st_h + l)\n",
    "    x_ranges = tf.range(-1 * d + st_w, -1 * d + st_w + l)\n",
    "\n",
    "    for i in range(0, hh//d+1):\n",
    "        s1 = i * d + st_h\n",
    "        s2 = i * d + st_w\n",
    "        y_ranges = tf.concat([y_ranges, tf.range(s1,s1+l)], axis=0)\n",
    "        x_ranges = tf.concat([x_ranges, tf.range(s2,s2+l)], axis=0)\n",
    "\n",
    "    x_clip_mask = tf.logical_or(x_ranges <0 , x_ranges > hh-1)\n",
    "    y_clip_mask = tf.logical_or(y_ranges <0 , y_ranges > hh-1)\n",
    "    clip_mask = tf.logical_or(x_clip_mask, y_clip_mask)\n",
    "\n",
    "    x_ranges = tf.boolean_mask(x_ranges, tf.logical_not(clip_mask))\n",
    "    y_ranges = tf.boolean_mask(y_ranges, tf.logical_not(clip_mask))\n",
    "\n",
    "    hh_ranges = tf.tile(tf.range(0,hh), [tf.cast(tf.reduce_sum(tf.ones_like(x_ranges)), tf.int32)])\n",
    "    x_ranges = tf.repeat(x_ranges, hh)\n",
    "    y_ranges = tf.repeat(y_ranges, hh)\n",
    "\n",
    "    y_hh_indices = tf.transpose(tf.stack([y_ranges, hh_ranges]))\n",
    "    x_hh_indices = tf.transpose(tf.stack([hh_ranges, x_ranges]))\n",
    "\n",
    "    y_mask_sparse = tf.SparseTensor(tf.cast(y_hh_indices, tf.int64),  tf.zeros_like(y_ranges), [hh, hh])\n",
    "    y_mask = tf.sparse.to_dense(y_mask_sparse, 1, False)\n",
    "\n",
    "    x_mask_sparse = tf.SparseTensor(tf.cast(x_hh_indices, tf.int64), tf.zeros_like(x_ranges), [hh, hh])\n",
    "    x_mask = tf.sparse.to_dense(x_mask_sparse, 1, False)\n",
    "\n",
    "    mask = tf.expand_dims( tf.clip_by_value(x_mask + y_mask, 0, 1), axis=-1)\n",
    "\n",
    "    mask = random_rotate(mask, rotate_angle, [hh, hh, 1])\n",
    "    mask = tf.image.crop_to_bounding_box(mask, (hh-h)//2, (hh-w)//2, image_height, image_width)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def apply_grid_mask(image, image_shape):\n",
    "    mask = GridMask(image_shape[0],\n",
    "                    image_shape[1],\n",
    "                    AugParams['d1'],\n",
    "                    AugParams['d2'],\n",
    "                    AugParams['rotate'],\n",
    "                    AugParams['ratio'])\n",
    "    \n",
    "    if image_shape[-1] == 3:\n",
    "        mask = tf.concat([mask, mask, mask], axis=-1)\n",
    "    return image * tf.cast(mask, tf.float32)\n",
    "\n",
    "    \n",
    "def augmentation(image):    \n",
    "    if tf.random.uniform(shape=[], minval=0.0, maxval=1.0) >=0.5:\n",
    "        image = apply_grid_mask(image, (*IMAGE_SIZE,3))        \n",
    "    return image#tf.cast(image, tf.uint8) #image, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Initialize dataset, visualization and augmentation functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_me = False\n",
    "if show_me:\n",
    "    dataset = get_training_dataset()\n",
    "    training_dataset = dataset.unbatch().batch(20)\n",
    "    train_batch = iter(training_dataset)\n",
    "    display_batch_of_images(next(train_batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 8e-05 to 0.00036 to 1.05e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning rate schedule for TPU, GPU and CPU.\n",
    "# Using an LR ramp up because fine-tuning a pre-trained model.\n",
    "# Starting with a high LR would break the pre-trained weights.\n",
    "def lrfn(epoch):\n",
    "    \n",
    "    LR_START = 0.00008\n",
    "#LR_START = 0.00001\n",
    "    LR_MAX = 0.000045 * strategy.num_replicas_in_sync\n",
    "    LR_MIN = 0.000008\n",
    "    LR_RAMPUP_EPOCHS = 3\n",
    "    LR_SUSTAIN_EPOCHS = 2\n",
    "    LR_EXP_DECAY = .77\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 8e-05 to 0.0004 to 1.01e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def esfn(epoch):\n",
    "    LR_START = 0.00008\n",
    "    LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
    "    LR_MIN = 0.000008\n",
    "    LR_RAMPUP_EPOCHS = 4\n",
    "    LR_SUSTAIN_EPOCHS = 0\n",
    "    LR_EXP_DECAY = .77\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "es_callback = tf.keras.callbacks.LearningRateScheduler(esfn, verbose = True)\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "y = [esfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 64104 training_1 images, 34477 training_2 images, 3712 validation images, 7382 unlabeled test images\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_TRAINING_IMAGES_model2 = count_data_items(TRAINING_FILENAMES_model2)\n",
    "\n",
    "NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n",
    "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "STEPS_PER_EPOCH_model2 = NUM_TRAINING_IMAGES_model2 // BATCH_SIZE\n",
    "\n",
    "print('Dataset: {} training_1 images, {} training_2 images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_TRAINING_IMAGES_model2,NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Begin downloading ENet B7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b7_noisy-student_notop.h5\n",
      "258072576/258068648 [==============================] - 17s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b7 (Model)      (None, 11, 11, 2560)      64097680  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 104)               266344    \n",
      "=================================================================\n",
      "Total params: 64,364,024\n",
      "Trainable params: 64,053,304\n",
      "Non-trainable params: 310,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# efficient net\n",
    "with strategy.scope():\n",
    "    enet = efn.EfficientNetB7(\n",
    "        input_shape=(*IMAGE_SIZE, 3),\n",
    "        weights='noisy-student',\n",
    "        include_top=False\n",
    "    )\n",
    "    enet.trainable = True\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        enet,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])\n",
    "        \n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish downloading ENet B7, start training ENet B7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 333 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 8e-05.\n",
      "Epoch 1/25\n",
      "333/333 [==============================] - 528s 2s/step - loss: 2.1833 - sparse_categorical_accuracy: 0.5008\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00017333333333333334.\n",
      "Epoch 2/25\n",
      "333/333 [==============================] - 210s 631ms/step - loss: 0.6731 - sparse_categorical_accuracy: 0.8295\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0002666666666666667.\n",
      "Epoch 3/25\n",
      "333/333 [==============================] - 210s 631ms/step - loss: 0.3837 - sparse_categorical_accuracy: 0.8989\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00036.\n",
      "Epoch 4/25\n",
      "333/333 [==============================] - 209s 626ms/step - loss: 0.2901 - sparse_categorical_accuracy: 0.9185\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00036.\n",
      "Epoch 5/25\n",
      "333/333 [==============================] - 210s 631ms/step - loss: 0.2196 - sparse_categorical_accuracy: 0.9383\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00036.\n",
      "Epoch 6/25\n",
      "333/333 [==============================] - 210s 631ms/step - loss: 0.1659 - sparse_categorical_accuracy: 0.9545\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027904000000000004.\n",
      "Epoch 7/25\n",
      "333/333 [==============================] - 212s 636ms/step - loss: 0.1186 - sparse_categorical_accuracy: 0.9670\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00021670080000000004.\n",
      "Epoch 8/25\n",
      "333/333 [==============================] - 212s 636ms/step - loss: 0.0828 - sparse_categorical_accuracy: 0.9775\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00016869961600000002.\n",
      "Epoch 9/25\n",
      "333/333 [==============================] - 211s 635ms/step - loss: 0.0615 - sparse_categorical_accuracy: 0.9809\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00013173870432000003.\n",
      "Epoch 10/25\n",
      "333/333 [==============================] - 210s 631ms/step - loss: 0.0467 - sparse_categorical_accuracy: 0.9861\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00010327880232640002.\n",
      "Epoch 11/25\n",
      "333/333 [==============================] - 213s 639ms/step - loss: 0.0392 - sparse_categorical_accuracy: 0.9861\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 8.136467779132802e-05.\n",
      "Epoch 12/25\n",
      "333/333 [==============================] - 212s 636ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9902\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.449080189932258e-05.\n",
      "Epoch 13/25\n",
      "333/333 [==============================] - 212s 636ms/step - loss: 0.0266 - sparse_categorical_accuracy: 0.9942\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 5.1497917462478383e-05.\n",
      "Epoch 14/25\n",
      "333/333 [==============================] - 213s 638ms/step - loss: 0.0223 - sparse_categorical_accuracy: 0.9959\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 4.1493396446108354e-05.\n",
      "Epoch 15/25\n",
      "333/333 [==============================] - 210s 631ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9937\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 3.378991526350344e-05.\n",
      "Epoch 16/25\n",
      "333/333 [==============================] - 212s 636ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9962\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 2.7858234752897646e-05.\n",
      "Epoch 17/25\n",
      "333/333 [==============================] - 212s 637ms/step - loss: 0.0167 - sparse_categorical_accuracy: 0.9949\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2.3290840759731186e-05.\n",
      "Epoch 18/25\n",
      "333/333 [==============================] - 210s 632ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9957\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1.9773947384993014e-05.\n",
      "Epoch 19/25\n",
      "333/333 [==============================] - 209s 629ms/step - loss: 0.0152 - sparse_categorical_accuracy: 0.9945\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.706593948644462e-05.\n",
      "Epoch 20/25\n",
      "333/333 [==============================] - 209s 628ms/step - loss: 0.0141 - sparse_categorical_accuracy: 0.9962\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.4980773404562358e-05.\n",
      "Epoch 21/25\n",
      "333/333 [==============================] - 211s 634ms/step - loss: 0.0130 - sparse_categorical_accuracy: 0.9972\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.3375195521513015e-05.\n",
      "Epoch 22/25\n",
      "333/333 [==============================] - 209s 628ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9976\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.2138900551565023e-05.\n",
      "Epoch 23/25\n",
      "333/333 [==============================] - 209s 628ms/step - loss: 0.0104 - sparse_categorical_accuracy: 0.9974\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.1186953424705067e-05.\n",
      "Epoch 24/25\n",
      "333/333 [==============================] - 209s 629ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9974\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.0453954137022901e-05.\n",
      "Epoch 25/25\n",
      "333/333 [==============================] - 210s 630ms/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    get_training_dataset(), \n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[lr_callback],\n",
    "    validation_data=None if SKIP_VALIDATION else get_validation_dataset()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish training ENet B7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VALIDATION:\n",
    "    display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n",
    "    display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Begin downloading DenseNet 201\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74842112/74836368 [==============================] - 1s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet201 (Model)          (None, 10, 10, 1920)      18321984  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 104)               199784    \n",
      "=================================================================\n",
      "Total params: 18,521,768\n",
      "Trainable params: 18,292,712\n",
      "Non-trainable params: 229,056\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    dnet = DenseNet201(\n",
    "        input_shape=(*IMAGE_SIZE, 3),\n",
    "        weights='imagenet',\n",
    "        include_top=False\n",
    "    )\n",
    "    dnet.trainable = True\n",
    "\n",
    "    model2 = tf.keras.Sequential([\n",
    "        dnet,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])\n",
    "        \n",
    "model2.compile(\n",
    "    optimizer= tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "model2.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish downloading DenseNet 201, start training DenseNet 201\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 179 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 8e-05.\n",
      "Epoch 1/25\n",
      "179/179 [==============================] - 356s 2s/step - loss: 1.3468 - sparse_categorical_accuracy: 0.7237\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00016.\n",
      "Epoch 2/25\n",
      "179/179 [==============================] - 76s 425ms/step - loss: 0.3337 - sparse_categorical_accuracy: 0.9276\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00024000000000000003.\n",
      "Epoch 3/25\n",
      "179/179 [==============================] - 74s 416ms/step - loss: 0.1758 - sparse_categorical_accuracy: 0.9567\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00032.\n",
      "Epoch 4/25\n",
      "179/179 [==============================] - 76s 426ms/step - loss: 0.1420 - sparse_categorical_accuracy: 0.9609\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 5/25\n",
      "179/179 [==============================] - 76s 427ms/step - loss: 0.1310 - sparse_categorical_accuracy: 0.9688\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00030984000000000003.\n",
      "Epoch 6/25\n",
      "179/179 [==============================] - 77s 429ms/step - loss: 0.0557 - sparse_categorical_accuracy: 0.9898\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00024041680000000003.\n",
      "Epoch 7/25\n",
      "179/179 [==============================] - 77s 429ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9951\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00018696093600000004.\n",
      "Epoch 8/25\n",
      "179/179 [==============================] - 77s 430ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9979\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00014579992072000003.\n",
      "Epoch 9/25\n",
      "179/179 [==============================] - 77s 430ms/step - loss: 0.0077 - sparse_categorical_accuracy: 0.9979\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00011410593895440002.\n",
      "Epoch 10/25\n",
      "179/179 [==============================] - 77s 430ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9995\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 8.970157299488802e-05.\n",
      "Epoch 11/25\n",
      "179/179 [==============================] - 77s 430ms/step - loss: 0.0042 - sparse_categorical_accuracy: 0.9986\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 7.091021120606377e-05.\n",
      "Epoch 12/25\n",
      "179/179 [==============================] - 77s 431ms/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 5.644086262866911e-05.\n",
      "Epoch 13/25\n",
      "179/179 [==============================] - 77s 430ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 4.529946422407521e-05.\n",
      "Epoch 14/25\n",
      "179/179 [==============================] - 76s 425ms/step - loss: 0.0017 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 3.6720587452537916e-05.\n",
      "Epoch 15/25\n",
      "179/179 [==============================] - 75s 420ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9998\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 3.0114852338454193e-05.\n",
      "Epoch 16/25\n",
      "179/179 [==============================] - 76s 427ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 2.5028436300609727e-05.\n",
      "Epoch 17/25\n",
      "179/179 [==============================] - 76s 427ms/step - loss: 0.0014 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2.111189595146949e-05.\n",
      "Epoch 18/25\n",
      "179/179 [==============================] - 77s 429ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1.8096159882631512e-05.\n",
      "Epoch 19/25\n",
      "179/179 [==============================] - 77s 430ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.5774043109626263e-05.\n",
      "Epoch 20/25\n",
      "179/179 [==============================] - 77s 429ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.3986013194412222e-05.\n",
      "Epoch 21/25\n",
      "179/179 [==============================] - 76s 427ms/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.2609230159697412e-05.\n",
      "Epoch 22/25\n",
      "179/179 [==============================] - 77s 430ms/step - loss: 0.0010 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.1549107222967007e-05.\n",
      "Epoch 23/25\n",
      "179/179 [==============================] - 77s 431ms/step - loss: 0.0010 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.0732812561684596e-05.\n",
      "Epoch 24/25\n",
      "179/179 [==============================] - 77s 428ms/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.0104265672497138e-05.\n",
      "Epoch 25/25\n",
      "179/179 [==============================] - 75s 419ms/step - loss: 8.7098e-04 - sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(\n",
    "    get_training_dataset_model2(), \n",
    "    steps_per_epoch=STEPS_PER_EPOCH_model2,\n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[es_callback],\n",
    "    validation_data=None if SKIP_VALIDATION else get_validation_dataset()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish training DenseNet 201\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VALIDATION:\n",
    "    display_training_curves(history2.history['loss'], history2.history['val_loss'], 'loss', 211)\n",
    "    display_training_curves(history2.history['sparse_categorical_accuracy'], history2.history['val_sparse_categorical_accuracy'], 'accuracy', 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VALIDATION:\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "    def show_report(model):\n",
    "        val_dataset = load_dataset(VALIDATION_FILENAMES, labeled=False, ordered=True)\n",
    "\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "        val_dataset = val_dataset.prefetch(AUTO)\n",
    "        #val_images_ds = val_dataset.map(normalize_testset)\n",
    "        val_images_ds = val_dataset.map(lambda image, idnum: image)\n",
    "\n",
    "        val_dataset2 = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=True)\n",
    "        val_labels_ds = [ label.numpy() for image, label in val_dataset2]\n",
    "        # val_names_ds = val_labels_ds.map(lambda idnum: CLASSES[idnum])\n",
    "\n",
    "        print(\"[INFO] evaluating...\")\n",
    "        probabilities = model.predict(val_images_ds)\n",
    "        predictions = np.argmax(probabilities, axis=-1)\n",
    "        print(classification_report(val_labels_ds, predictions, target_names=CLASSES))\n",
    "\n",
    "        # # compute the raw accuracy with extra precision\n",
    "        acc = accuracy_score(val_labels_ds, predictions)\n",
    "        print(\"[INFO] score: {}\".format(acc))\n",
    "\n",
    "    show_report(model)\n",
    "    show_report(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VALIDATION: \n",
    "    cmdataset = get_validation_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and labels, order matters.    \n",
    "    images_ds = cmdataset.map(lambda image, label: image)\n",
    "    labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n",
    "    cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() # get everything as one batch\n",
    "    m = model.predict(images_ds)\n",
    "    m2 = model2.predict(images_ds)\n",
    "    scores = []\n",
    "    for alpha in np.linspace(0,1,100):\n",
    "        cm_probabilities = alpha*m+(1-alpha)*m2\n",
    "    #cm_probabilities = (model.predict(images_ds)*0.3)+model2.predict(images_ds)*0.7 \n",
    "        cm_predictions = np.argmax(cm_probabilities, axis=-1)\n",
    "        scores.append(f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro'))\n",
    "    print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n",
    "    print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)\n",
    "    plt.plot(scores)\n",
    "    best_alpha = np.argmax(scores)/100\n",
    "    cm_probabilities = best_alpha*m  +  (1-best_alpha)*m2\n",
    "    cm_predictions = np.argmax(cm_probabilities, axis=-1)\n",
    "else:\n",
    "    best_alpha = 0.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53\n"
     ]
    }
   ],
   "source": [
    "print(best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VALIDATION:\n",
    "    cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\n",
    "    score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
    "    precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
    "    recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
    "    #cmat = (cmat.T / cmat.sum(axis=1)).T # normalized\n",
    "    display_confusion_matrix(cmat, score, precision, recall)\n",
    "    print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Start calculating predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing predictions...\n",
      "[ 67  28  83 ...  95 102  62]\n",
      "Generating submission.csv file...\n"
     ]
    }
   ],
   "source": [
    "test_dataset = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n",
    "\n",
    "print('Computing predictions...')\n",
    "test_images_ds = test_dataset.map(lambda image, idnum: image)\n",
    "probabilities = best_alpha*model.predict(test_images_ds) + (1-best_alpha)*model2.predict(test_images_ds)\n",
    "predictions = np.argmax(probabilities, axis=-1)\n",
    "print(predictions)\n",
    "\n",
    "print('Generating submission.csv file...')\n",
    "test_ids_ds = test_dataset.map(lambda image, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
    "np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.00s - Start notebook\n",
      "  22.01s - Import libraries, initilialize TPU and config\n",
      "  22.16s - Initialize dataset, visualization and augmentation functions\n",
      "  22.58s - Begin downloading ENet B7\n",
      " 112.89s - Finish downloading ENet B7, start training ENet B7\n",
      "5698.45s - Finish training ENet B7\n",
      "5698.47s - Begin downloading DenseNet 201\n",
      "5763.31s - Finish downloading DenseNet 201, start training DenseNet 201\n",
      "7956.97s - Finish training DenseNet 201\n",
      "7957.04s - Start calculating predictions\n",
      "8199.08s - Finish calculating predictions\n"
     ]
    }
   ],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish calculating predictions\")\n",
    "\n",
    "for event in exec_history:\n",
    "    print(f'{event[1]:7.2f}s - {event[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
